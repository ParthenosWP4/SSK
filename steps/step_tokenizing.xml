<?xml version="1.0" encoding="UTF-8"?>
<?xml-model href="https://raw.githubusercontent.com/ParthenosWP4/SSK/master/spec/TEI_SSK_ODD.rng" type="application/xml" schematypens="http://relaxng.org/ns/structure/1.0"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0"
     type="researchStep"
     xml:id="step_tokenizing">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <title>Step \"Tokenizing\"</title>
            <author>
               <forename>Matej</forename>
               <surname>Ďurčo</surname>
               <affiliation>OEAW-ACDH</affiliation>
            </author>
            <author>
               <forename>Tom</forename>
               <surname>Gheldof</surname>
               <affiliation>KU Leuven, DARIAH-BE</affiliation>
            </author>
            <author>
               <forename>Hannes</forename>
               <surname>Pirker</surname>
               <affiliation>OEAW-ACDH</affiliation>
            </author>
            <author>
               <forename>Gerlinde</forename>
               <surname>Schneider</surname>
               <affiliation>ZIM-ACDH</affiliation>
            </author>
            <sponsor>DARIAH-EU</sponsor>
         </titleStmt>
         <publicationStmt>
            <authority>DARIAH-EU</authority>
            <availability>
               <licence target="http://creativecommons.org/licenses/by/4.0/">
                  <p>The Creative Commons Attribution 4.0 Unported (CC BY 4.0) Licence applies
                            to this document.</p>
               </licence>
            </availability>
         </publicationStmt>
         <sourceDesc>
            <p>Created from scratch</p>
         </sourceDesc>
      </fileDesc>
      <revisionDesc><!-- Use only for major changes -->
         <change/>
      </revisionDesc>
   </teiHeader>
   <text>
      <body>
         <listEvent>
            <event type="researchStep">
               <head type="stepTitle" xml:lang="en">Tokenizing</head>
               <desc type="definition" xml:lang="en">Tokenizing a text refers to the task of
                        splitting it up into identifiable units -- the tokens. Or to state it in
                        less formal terms: to split the text into words and punctuation marks.
                        Tokenization is a prerequisite for basically all other forms of linguistic
                        analysis, as it provides the basic units on which to operate. Though some
                        "higher level" NLP tools would allow to use plain text as input i.e. deal
                        with the necessary tokenization themselves, many tools still rely on
                        tokenized text. Tokenizers are typically quite simple programs: the basic
                        heuristic is to split the text at whitespaces and punctuation characters. An
                        important factor in many western languages is to properly distinguish
                        punctuation characters which are actually part of an abbreviation like in
                        "e.g.". The quality of many tokenizers thus typically relies on the
                        application of language specific abbreviation lists and regular expressions
                        for correctly identifying sequences of characters which form a token. E.g.
                        web and e-mail addresses, date and time etc. </desc>
               <desc xml:lang="en" type="terms">
                  <term type="standard" source="standard_list" key="XML"/>
                  <term type="standard" source="standard_list" key="TEI"/>
                  <term type="standard" source="standard_list" key="TCF"/>
                  <term type="standard" source="standard_list" key="LAF"/>
                  <term type="activity" source="http://tadirah.dariah.eu/" key="annotating"/>
               </desc>
               <linkGrp type="generalResources"
                        xml:base="http://www.zotero.org/groups/427927/ssk-parthenos/items/itemKey/">
                  <ref type="software" source="zotero" target="K3LB6TK5"><!-- Unitok tokenizer tool  -->
                     <term type="software" key="Unitok"/>
                  </ref>
               </linkGrp>
            </event>
         </listEvent>
      </body>
   </text>
</TEI>
